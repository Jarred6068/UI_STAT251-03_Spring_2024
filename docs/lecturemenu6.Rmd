---
title: "Week 6 Notes"
author: "STAT 251 Section 03"
output:
  html_document: default
  #pdf_document: default
always_allow_html: true
header-includes:
  \usepackage{float}
  \usepackage{tikz}

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(knitr)
library(kableExtra)
library(ggplot2)
library(ggthemes)
library(ggpubr)
library(latex2exp)

path = 'C:/Users/Bruin/Documents/GitHub/UI_STAT251-03_Spring_2024/docs/Data/'
```

# Lecture 10: Monday, Feb.12th 2024

### Sources of Bias In Surveys

Last Friday, we introduced the idea of <i>randomization</i> as a means of selecting a sample from the population through a sampling design called **Simple random sampling**. Using random numbers to select individuals that are selected to be in the sample are representative of the true population. However, large surveys of the human population require more than just sampling design with randomization. 

A good survey needs an accurate sampling frame for the population, which is rarely the case in surveys of large human populations. As a result, most surveys suffer from **undercoverage** - bias that is introduced from having an inaccurate sampling frame. For example, consider a survey of American households. Such a survey will not include homeless individuals, college students, or incarcerated persons. Thus population surveys usually have some degree of bias introduced by the individuals not included by the sampling frame.

Other sources of bias in surveys include:

**Nonresponse bias** - this is a more serious form of bias that occurs when individuals selected to be in the sample cannot be contacted or refuse to participate. 
  
  * For many sample survey's nonresponse bias is a considerable issue reaching $50\%$ or more of selected individuals.
    
  * nonresponse bias is typically higher in urban areas
    
  * An important thing to keep in mind is that nearly all political and opinion polls reported in the news fail to report their nonresponse rates.  

**Response bias** - response bias occurs when the interview portion of a survey influences how individuals respond to the survey questions. Response bias can include:

  * Poorly worded questions which may confuse respondents and influence how they select their answer. 
    
  * Questions about sensitive topics, such as illict drug use, in which respondants may lie about their behavior.
    
  * The race or sex of the interviewer can influence responses related to questions about race relations or feminism. 

### More Complicated Sampling Designs

**Systematic sampling** – A random sampling method in which the researcher selects every $k^{th}$ subject from an ordered sampling frame
 
  * This method of sampling is useful when there is no sampling frame available.
    
  * Estimates obtained under this sampling design typically have a lower margin of error than simple random sampling and some cluster sampling designs.

 
**Cluster sampling** – A type of sampling method in which the population is divided in a set of clusters and the researcher selects a simple random sample of the clusters. The sample then comprises all subjects in the selected clusters.
 
  * The advantages of cluster sampling are that (a) it can be less expensive than simple or stratified random sampling and (b) it can be used when a sampling frame is unavailable 

  * A disadvantage of cluster sampling is that the margin of error is often larger than what it would be for simple random sampling or stratified random sampling


**Stratified random sampling** – A type of sampling method in which the population is separated into groups, call strata, based on some characteristic about the subjects. A simple random sample is then taken from each stratum. 

  * Administrative convenience - It may be easier to conduct several smaller simple random sampling designs than coordinate one larger simple random sampling design.

  * Interest in individual strata - The design ensures samples from all strata. A simple random sampling design might sample few or no elements from a stratum of interest.

  * Smaller margin of error - By assuring samples from each strata, the combined sample tends to be more representative of the population, resulting in a smaller margin of error




![**Figure 1** - visual depiction of sampling designs](C:/Users/Bruin/Documents/GitHub/UI_STAT251-03_Spring_2024/docs/misc/Sampling_designs.png)

<br>
<br>
**Two – stage cluster sampling** - A type of sampling method in which the population is divided into a set of clusters and the researcher selects a simple random sample of the clusters. A simple random sample is then applied to each cluster
 
  * Same advantages as cluster sampling
    
  * Usually has a smaller margin of error, because we can control two sample sizes: the number of clusters to sample, and the number of elements to sample from each sampled cluster


![**Figure 2** - visual depiction of two-state cluster sampling](C:/Users/Bruin/Documents/GitHub/UI_STAT251-03_Spring_2024/docs/misc/two_stage_cs.png)


<br>
<br>

### Ethical studies

Producing and using data to answer statistical questions may not seem like a big deal, but, like all human endeavors, it raises ethical concerns. The most prominent ethical issues arise when we collect data about people. Ethical considerations must be closely contemplated when we conduct experiments using human subjects. While an experiment may have potentially life saving benefits, it can also come with considerable risks that are bore solely by the participating subjects. Some ethical considerations that apply to any study are:

* Informed consent - subjects participating in a survey or experiment are usually required to provide legal consent before data can be collected

* Confidentiality - many studies on human subjects require that the data gather be kept confidential with proper security infrastructure in place to protect it. 

    * Can you think of some examples of areas of study that may require confidentiality or informed consent?

The University of Idaho, like all colleges and universities engaged in research, have a review board who is responsible for establishing ethics guidelines for conducting research and review studies which may require ethical considerations. The University of Idahos Office of Research Assurances website can be found [here](https://www.uidaho.edu/research/faculty/research-assurances)


The following [link](https://www.esri.com/arcgis-blog/products/survey123/constituent-engagement/ethics/) from the Environmental Systems Research Institute, inc website provides a great overview of the survey ethics, design and implementation. 

Similarly, this [link](https://www.nih.gov/health-information/nih-clinical-research-trials-you/guiding-principles-ethical-research) from the National Institutes of Health provides a brief overview of the guiding principles of ethical research. 

### Connection with statistical inference

Now that we have learned some different ways to conduct experimental studies and surveys, how does this connect with our ultimate goal of inferring things about a population, or making informed decisions? Recall that a **sampling distribution** is the distribution of a statistic that arises from taking <u>all possible samples</u> of the same size $n$ from the same population. 

If we think of the population parameter $\theta$ as the bullseye of a target, then, ideally we would like our estimator $\hat{\theta}$ to fall close to or on the value of $/theta$. However, how we sample or conduct an experiment can influence the shape, center, and spread of the sampling distribution of a statistic. 

* <i>Bias of a statistic</i> - concerns the center of a sampling distribution. A statistic is **unbiased** if the mean of its sampling distribution is equal to the true value of the population parameter.

* <i>Variability of a statistic</i> - concerns the spread of its sampling distribution. The spread <u>is determined by the sampling design</u> and the sample size $n$. Larger sample sizes have smaller spreads. 

```{r, echo=F, message=F, warning=F, fig.height=5, fig.width=5}
set.seed(123)
n = 50
library(ggforce)

x1 = rnorm(n, 0, 0.5)
y1 = rnorm(n, 0, 0.5)
A = ggplot()+geom_point(aes(x = x1, y = y1), shape = 21, fill = 'red', size = 3)+
  theme_void()+
  geom_circle(aes(x0 = 0, y0 = 0, r = 3), size = 1)+
  geom_circle(aes(x0 = 0, y0 = 0, r = 2), size = 1)+
  geom_circle(aes(x0 = 0, y0 = 0, r = 1), size = 1)+
  geom_text(aes(x = 0, y = 0), label = TeX('$\\theta$'), size = 12)+
  ggtitle('No Bias')+
  ylab('Low Variance')+
  theme(axis.title.y = element_text(14, angle = 90),
        plot.title = element_text(hjust = 0.5))

x2 = rnorm(n, 1, 0.5)
y2 = rnorm(n, 1, 0.5)
B = ggplot()+geom_point(aes(x = x2, y = y2), shape = 21, fill = 'red', size = 3)+
  theme_void()+
  geom_circle(aes(x0 = 0, y0 = 0, r = 3), size = 1)+
  geom_circle(aes(x0 = 0, y0 = 0, r = 2), size = 1)+
  geom_circle(aes(x0 = 0, y0 = 0, r = 1), size = 1)+
  geom_text(aes(x = 0, y = 0), label = TeX('$\\theta$'), size = 12)+
  ggtitle('Biased')+
  ylab('')+
  theme(axis.title.y = element_text(14),
        plot.title = element_text(hjust = 0.5))

x3 = rnorm(n, 0, 1)
y3 = rnorm(n, 0, 1)
C = ggplot()+geom_point(aes(x = x3, y = y3), shape = 21, fill = 'red', size = 3)+
  theme_void()+
  geom_circle(aes(x0 = 0, y0 = 0, r = 3), size = 1)+
  geom_circle(aes(x0 = 0, y0 = 0, r = 2), size = 1)+
  geom_circle(aes(x0 = 0, y0 = 0, r = 1), size = 1)+
  geom_text(aes(x = 0, y = 0), label = TeX('$\\theta$'), size = 12)+
  ggtitle('')+
  ylab('High Variance')+
  theme(axis.title.y = element_text(14, angle = 90),
        plot.title = element_text(hjust = 0.5))

x4 = rnorm(n, 1, 1)
y4 = rnorm(n, 1, 1)
D = ggplot()+geom_point(aes(x = x4, y = y4), shape = 21, fill = 'red', size = 3)+
  theme_void()+
  geom_circle(aes(x0 = 0, y0 = 0, r = 3), size = 1)+
  geom_circle(aes(x0 = 0, y0 = 0, r = 2), size = 1)+
  geom_circle(aes(x0 = 0, y0 = 0, r = 1), size = 1)+
  geom_text(aes(x = 0, y = 0), label = TeX('$\\theta$'), size = 12)+
  ggtitle('')+
  ylab('')+
  theme(axis.title.y = element_text(14),
        plot.title = element_text(hjust = 0.5))

ggarrange(A, B, C, D, nrow =2, ncol = 2)

```




The variation between samples means that estimates or values of a statistic are doomed to depart somewhat from the true population parameter simply by chance. The chance difference is called **sampling error** – which is how close our estimate is from the true population parameter 
    
  * Sampling error should not to be confused with the <i>margin of error</i> which is the maximum error that we might observe

```{r, echo=F, message=F, warning=F, fig.height=5, fig.width=8}

set.seed(532)
x = rnorm(1000, 5, 3)
ss = x[sample(1:1000, 10)]
k = ceiling(sqrt(1000))
ggplot()+geom_histogram(aes(x = x, y = ..density..), bins = k, fill = 'lightgrey', color = 'black')+
  theme_classic2()+
  geom_vline(xintercept = mean(x)+2*sd(x), color = 'blue', size = 1.5, linetype = 'dotted')+
  geom_vline(xintercept = mean(x)-2*sd(x), color = 'blue', size = 1.5, linetype = 'dotted')+
  geom_segment(aes(x = mean(ss)+3, y = 0, xend = mean(ss)+3, yend = 0.15), 
               color = 'red', size = 2, linetype = 'dotted')+
  geom_segment(aes(x = mean(x)+2*sd(x), y = 0.05, xend= mean(x)-2*sd(x), yend = 0.05), 
               size = 1.5, color = 'blue', 
               arrow = arrow(length = unit(0.03, "npc"), ends = "both"))+
  geom_segment(aes(x = mean(x), y = 0, xend= mean(x), yend = 0.15), 
               size = 1.5, color = 'black', linetype = 'dashed')+
  geom_segment(aes(x = mean(x), y = 0.1, xend= mean(ss)+3, yend = 0.1), 
               size = 1.5, color = 'red',
               arrow = arrow(length = unit(0.03, "npc"), ends = "both"))+
  geom_text(aes(x = mean(x), y = 0.155), label = TeX('$\\theta$'), size = 8)+
  geom_text(aes(x = mean(ss)+3, y = 0.155), label = TeX('$\\hat{\\theta}$'), size = 8,
            color = 'red')+
  geom_label(aes(x = mean(x) + (mean(ss)+3)/2, y = 0.11), label = 'Sampling Error', size = 5,
            color = 'red')+
  geom_label(aes(x = mean(x), y = 0.06), label = 'Margin of Error', size = 5,
            color = 'blue')+
  theme(axis.title = element_blank(),
        axis.text = element_blank(),
        axis.ticks = element_blank())
  

```


So why worry about sampling design?

* randomization guarantees that the results we get from analyzing our data follow the <u> laws of probability</u>. 

* Things like surveys and experiments cost precious resources like money, time, and effort. So when we estimate a population parameter, we want our estimate to be as precise as possible - i.e we want the margin of error to be as small as possible. 

Under a randomized experiment or survey, we can reduce variability in the sampling distribution of a statistic by simply selecting a larger sample size. In fact, we can make the sampling error as small as we would like or at least as small as our bank accounts will allow!

* And we can often further reduce variability in our estimate by choosing a design that accounts for things like subject or sample heterogeneity.  


# Lecture 11: Wednesday, Feb.14th 2024

### Introduction to probability

The reasoning of statistical inference relies on asking "how often would this method give the correct result if I used it many times?". When we produce data by random sampling or through randomized experiments, the outcomes of our analysis are determined by the rules of mathematical probability. 

Tossing a coin, rolling a die, the weather, the change in stock market, or choosing a simple random sample, these are all <i>random phenomena</i> because their results cannot be predicted with $100/%$ accuracy each time. However, for each random phenomena there will be predictable patterns in the results. This is the basis of the idea of mathematical probability. An important note is that probability describes only the long-term frequencies or patterns of events. It is much less useful for quanitifying the chance outcomes of events in the short term. 

### The language of probability

* **Probability** is a measurement of the “likelihood” of an event as a number between 0 and 1. It describes the long-term frequency with which an event might occur. For example, we describe the probability of flipping a coin and getting heads as the number 0.5, which describes the chance that of the event of getting heads, assuming the coin is fair $50\%$.

* An **event** is any possible outcome of a random phenomena or experiment. For example, the events that might occur when flipping a coin are getting heads or tails. 

* The act of flipping a coin once is called a **random trial** - a process or experiment that has a set of well – defined possible outcomes. What distinguishes it as random is that there is more than one possible outcome e.g heads OR tails. What are the possible outcomes of rolling a single six-sided die? 

* We call the set of all possible outcomes of random phenomena the **sample space**. It is common in probability theory to use capital $S = \{\}$ to denote the sample space mathematically. For example, the sample space of flipping a coin once is $S = \{\text{Heads}, \text{Tails} \}$. The sample of a single roll of a six-side die is $S = \{1,2,3,4,5,6\}$.

* as a general rule, the probability of an event $A$ denoted $P(A)$ is:

\[ P(A) = \frac{\text{Number of ways $A$ can happen}}{\text{Total number of outcomes}} \]

The denominator in the fraction of above is the sample space and the numerator represents the fraction of the sample space that includes $A$

Consider rolling a fair single six-sided die. What is the probability that you would roll the number of six? What is the probability that you would not roll a six?. Knowing that the die is fair tells us that each number has an equal probability of occurring. Thus rolling any particular number on the die has a probability of $1/6 = 0.167$ and the probability of rolling any number but a six has to be $1-(1/6) = 5/6 = 0.833$. This simple example demonstrates several of the rules of probability:

* 1. $0\leq P(A)\leq 1$ - This rule states that a valid probability must be a value between zero and one.

<br>

* 2. $\sum_{i\in S} P(A_i) = 1$ - This rule states probability of the entire sample space must be one. 

<br>

* 3. $P(A \cup B ) = P(A)+P(B)$ - The two events $A$ and $B$ are said to be **disjoint** if they have no outcomes in common. For two disjoint events $A$ and $B$, the probability that either event occurs is the sum of their individual probabilities. 

<br>

* 4. $P(A') = 1-P(A)$ - This rule states that the probability of the complement of an event $A$ is one minus the probability of the event $A$. The symbol $A'$ means complement of $A$ or “$A$-not” and means a number or quantity required to make a group whole.

Example: A standard deck of $52$ playing cards contains cards from four suits: hearts, diamonds, clubs, and spades. Each suit consists of 13 cards: an Ace, the cards with the numbers 2-10, a jack, a queen and a king card. Suppose you are playing a hand of poker and the dealer is about to deal everyone their first card. What is the probability that the card you are dealt is not a queen?

* the probability that the card is a queen is $4/52$ because there are four suits and each has one queen. Thus using rule three above we can conclude that the probability of NOT being dealt a queen card is $1-\frac{4}{52} = \frac{48}{52}$. 

### Dealing with multiple events

We are often interested in the probability of more than one event occurring simultaneously. The language of probability gives us tools to deal with outcomes of multiple events. A **union** of two or more events is that probability that any of the event occurs. If the events are disjoint then we can use rule $3$ above. If the events have outcomes in common then we must be sure not to double count the probability of their overlap. For two non-disjoint events $A$ and $B$:

\[ P(A\cup B) = P(A)+P(B)-P(A\cap B) \]

the quantity $P(A\cap B)$ is called the <i>intersection</i> of events $A$ and $B$ and represents the fraction of the sample space that $A$ and $B$ have in common. In general, the intersection of two independent events is the probability that both events occur:

\[P(A\cap B) = P(A)\cdot P(B) \]

To illustrate this consider the following example: Some states are considering laws that will ban the use of cell phones while driving because they believe that the ban will reduce the number of phone-related accidents. One study classified these types of accidents by the day of the week when they occurred \cite{}. For this example, we will use the values from this study as our probability model. Here are the probabilities 

```{r, echo=FALSE, warning=F, message=FALSE}
df = cbind.data.frame(Day = c('Sun.', 'Mon.', 'Tues.', 'Wed.','Thurs.', 'Fri.', 'Sat.'),
                      Probability = c(0.03, 0.19, 0.18, 0.23, 0.19, 0.16, 0.02))

kable(df, format = 'html', digits = 2, row.names = F, booktabs = T, escape = F)%>%
  kable_classic(full_width = F, html_font = "Cambria")%>%kable_styling(bootstrap_options = 'striped')
```

What is the probability that an accident occurs on a weekend, that is on Saturday or Sunday?

\[ P(\text{Saturday or Sunday}) = P(\text{Saturday})+P(\text{Sunday}) = 0.02+0.03 = 0.05 \]

what is the probability that an individudal gets into an accident on Friday and again on Monday?

\[ P(\text{Friday and Monday}) = P(\text{Friday})\cdot P(\text{Monday}) = 0.16\cdot 0.19 = 0.03\]


### Where do probabilities come from?

Think back to flipping a fair coin a single time, what is the probability that we get “heads”? Our intuition tells us that it is going to be $1/2$ if we have an equal chance of getting heads or tails, but how can we be sure that this is the likelihood of getting heads? Suppose that we flip a fair coin and it comes up tails, does this mean our intuition was wrong? 

Now imagine flipping a coin 4 times. We expect to see half of the flips be heads and half of the flips tails. However, we might see heads come up 3 times and tails once. The proportion of times we see heads is now $75\%$ which is much greater than our expectation of $50\%$.

In both cases, the result defied our expectation but this is because the outcome of any small number of trials can vary quite a bit from our expectations. Imagine flipping a coin $10,000$ times. The result would be a proportion of heads very close to $0.5$. As we stated earlier, with random phenomena, the proportion of times that an event happens can be quite unpredictable in the short run, but very predictable in the long run. Thus probabilities quantify the "long-run" behavior or frequency of a particular event. 

This long-run behavior is established by a property of mathematical probability called the **Law of Large Numbers** (LLN). The LLN states that the relative frequency of an event will tend to “approach” (in some sense) the probability of an event as the number of independent observations increases

```{r, echo=F, message=F, warning=F, fig.height=5, fig.width=10, fig.cap='Figure: A demonstration of the Law of Large Numbers. The plot below represents the running proportion of fair coin flips that come up heads for a large number of repeated independent trails. The red line indicates the expected proportion of $P(Heads) = 0.5$'}

set.seed(432)
S = 5000
p = 0.5
n = 1

LLNbern<-function(S,p,n){
  #S should be the max sample
  #p is the probability of the outcome
  #n is the number of trials in the experiment (for bernoulli = 1)
  #NOTE: when n > 1 this function transfers to a binomial exper.
  
  avg=rep(0, S)
  expected = n*p
  positive.outcomes=cumsum(rbinom(S, n, p))
  trials=c(1:S)
  avg=positive.outcomes/trials
  error = avg - expected
  variance = c(0,unlist(lapply(c(2:S), function(x,y)  var(y[1:x]), y = avg)))
  
  
  df = cbind.data.frame(Y = c(avg,variance), LN = c(rep("Running Prop.", S), rep("Var. In Prop.", S)),
                        SS = rep(trials, 2))
  
  
  return(df)
  
}

exp.bernoulli = LLNbern(S, p, n)

ggplot(subset(exp.bernoulli, LN == 'Running Prop.'), aes(x = SS, y = Y))+
  geom_line(size = 1)+
  theme_hc()+
  geom_hline(yintercept = p, color = 'red', size = 1.2)+
  xlab('Number of trials')+
  ylab('Relative Frequency of Heads')+
  ggtitle('Relative Frequency of Heads')

```

One assumption that we made in the coin flip example was that the trials were **independent trials**. This means that the outcome of one flip of the coin did not depend on the outcome of a previous flip. 

Practice: Consider rolling two fair six-sided die. What is the probability that 













