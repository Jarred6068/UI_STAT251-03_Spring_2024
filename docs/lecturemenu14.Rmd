---
title: "Week 14 Notes"
author: "STAT 251 Section 03"
output:
  html_document: default
  #pdf_document: default
always_allow_html: true
header-includes:
  \usepackage{float}
  \usepackage{tikz}
bibliography: C:/Users/Bruin/Documents/GitHub/UI_STAT251-03_Spring_2024/docs/misc/Week_13_bibs.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(knitr)
library(kableExtra)
library(ggplot2)
library(ggthemes)
library(ggpubr)
library(latex2exp)
library(gganimate)

path = 'C:/Users/Bruin/Documents/GitHub/UI_STAT251-03_Spring_2024/docs/Data/'

##stat pack
source('C:/Users/Bruin/Documents/GitHub/UI_STAT251-03_Spring_2024/docs/misc/stat251_tools.R')

```

# Lecture 26 Monday, April 8th 2024
  
### Comparing two means

Last week we learned how to compare two populations across a qualitative response variable by comparing their relative proportions. Now we will learn how we can compare two groups on a quantitative response variable by comparing their means. There are many cases where researchers might be interesting comparing groups across a quantitative variable. For example, a teacher may be interested in comparing the average test scores of students in two different classes (Class A and Class B). A social scientist may be interest in whether there is a difference in salaries between men and women within a company. In any case, we are again dealing with two samples of sizes $n_1$ and $n_2$ which come from populations <i>population 1</i> and <i>population 2</i> and with means as $\mu_1$ and $\mu_2$. 

```{r, echo=FALSE, warning=F, message=F}
set.seed(123)

df = cbind.data.frame(Population = c(1, 2),
                      `Population Mean` = c('$\\mu_1$', '$\\mu_2$'),
                      `Population Stdev` = c('$\\sigma_1$','$\\sigma_2$'),
                      `Sample Size` = c('$n_1$', '$n_2$'),
                      `Sample Mean` = c('$\\bar{x}_1$', '$\\bar{x}_2$'),
                      `Sample Stdev` = c('$s_1$','$s_2$'))

kable(df, format = 'html', digits = 2,
      row.names = F, booktabs = T, escape = F)%>%
  kable_classic(full_width = F, html_font = "Cambria")%>%kable_styling(bootstrap_options = 'striped')



```


Let $\mu_d = \mu_1 - \mu_2$ be the mean difference between the two populations. We can construct confidence intervals to estimate the average difference as well as conduct hypothesis tests to determine if two populations are significantly different.

### A confidence interval for the mean difference $\mu_d$

We will begin by assuming that we are dealing with two independent samples from two distinct populations. The natural estimator of $\mu_d$ is the difference between the sample means 

\[\hat{\mu}_d = \bar{x}_1 - \bar{x}_2\]

To base our inference on the above estimator, we must know its sampling distribution. If we assume that both samples come from populations that are normally distributed, then the distribution of the difference will also be normally distributed and the variances will add:

\[Var[\mu_1 - \mu_2] = \frac{\sigma_1^2}{n_1}+\frac{\sigma_2^2}{n_2}\]

In practice, $\sigma_1^2$ and $\sigma_2^2$ are unknown population parameters but can be estimated by substituting in the sample variances $s_1^2$ and $s_2^2$. This gives the following standard deviation for the estimator $\hat{\mu}_d$

\[SE(\hat{\mu}_d) = \sqrt{\frac{s_1^2}{n_1}+\frac{s_2^2}{n_2}}  \]

Just as it was for a single mean, estimating the population variances with the sample variances adds additional uncertainty. Thus, the the sampling distribution for the difference in two means is $t$-distributed, although the degrees of freedom are calculated differently. The actual formula for the degrees of freedom is quite messy and difficult to compute:

\[df = \frac{\left(\frac{s_1^2}{n_1}+\frac{s_2^2}{n_2}\right)^2}{ \frac{1}{n_1 - 1}\left(\frac{s_1^2}{n_1}\right)^2 + \frac{1}{n_2-1}\left(\frac{s_2^2}{n_2}\right)^2} \]

We will instead compute the degrees of freedom by taking the smaller of $n_1-1$ and $n_2 - 1$ which is a lower bound on the formula above 

\[\hat{\mu}_d \sim t(\min(n_1 - 1, n_2 - 1)) \]

The margin of error for estimating the mean difference at the $(1-\alpha)\%$ level is given by

\[m = t_{1-\alpha/2} \times SE(\hat{\mu}_d) \]

which gives the following confidence interval for the mean difference

\[(\bar{x}_1 - \bar{x}_2) \pm  t_{1-\alpha/2} \times SE(\hat{\mu}_d)\]

**Example: Comparing Customer Satisfaction Levels** - Suppose we want to compare the customer satisfaction levels of two competing cable television companies, Company 1 and Company 2. We randomly select customers from each company and ask them to rate their cable companies on a five-point scale (with 1 being least satisfied and 5 most satisfied). The data are summarized in the table below:


```{r, echo=FALSE, warning=F, message=F}
set.seed(123)

df = cbind.data.frame(`Group` = c('Company 1', 'Company 2'),
                      `Number of customers` = c(174, 355),
                      `Sample Mean Rating` = c(4.2, 3.8),
                      `Sample Stdev` = c(0.8, 1.1))

kable(df, format = 'html', digits = 2,
      row.names = F, booktabs = T, escape = F)%>%
  kable_classic(full_width = F, html_font = "Cambria")%>%kable_styling(bootstrap_options = 'striped')



```


Estimate the difference in mean satisfaction levels between the two companies at the $95\%$ confidence level.

### A significance test for comparing two means: independent samples

We can also test for a difference between two groups on a quantitative response. variable by comparing their means. The set up for a significance test for a difference in two means is more or less the same as we saw previously for two proportions, however, the calculations now involve $\bar{x}_1$, $\bar{x}_2$, $s_1$ and $s_2$. Recall that, under the assumption of normality for both populations and when using the sample standard deviations to estimate $\sigma_1$ and $\sigma_2$ the difference in means is $t$-distributed. We will see that test statistic $t_{obs}$ is the standardized difference and has approximately the same distribution. Hereâ€™s how we set up a two-sample hypothesis test for means:

**1. Assumptions**

  * Data constitute a simple random sample 
  
  * The two groups are independent
  
  * Both populations are normally distributed

**2. Null and Alternative Hypotheses**

  * The **null** hypothesis is generally that the two population means are equal (i.e no difference)
  
  \[H_0: \mu_1 = \mu_2 \ \ \  \text{which is the same as} \ \ \ H_0: \mu_1 - \mu_2 = 0\]
  
  * The alternative hypothesis can be either lower, upper, or two tailed just as it was with our univariate tests. The table below gives the possible alternative hypotheses and their critical values:
  
```{r, echo=FALSE, warning=F, message=F}
set.seed(123)

df = cbind.data.frame(`Alternative Hypothesis` = c('$H_A: \\mu_1 - \\mu_2 \\neq 0$','$H_A: \\mu_1 - \\mu_2 < 0$','$H_A: \\mu_1 - \\mu_2 > 0$'),
                      `Critical Value` = c('$t_{1-\\alpha/2}$','$t_\\alpha$','$t_{1-\\alpha}$'),
                      `Rejection Region` = c('$|t| \\geq t_{1-\\alpha/2}$',
                                             '$t\\leq t_{\\alpha}$',
                                             '$t\\geq t_{1-\\alpha}$'))

kable(df, format = 'html', digits = 2,
      row.names = F, booktabs = T, escape = F)%>%
  kable_classic(full_width = F, html_font = "Cambria")%>%kable_styling(bootstrap_options = 'striped')



```

**3. Test statistic**

  * Under the null hypotheses the test statistic is the standardized difference
  
  \[t_{obs} = \frac{(\bar{x}_1 - \bar{x}_2) - (\mu_1 - \mu_2)}{\sqrt{\frac{s_1^2}{n_1}+\frac{s_2^2}{n_2}}}  \]
  
  The above test statistic is only approximately $t$-distributed. However, the lower bound for the degrees of freedom given by $\min(n_1 - 1, n_2 - 1)$ generally leads to conservative estimates and tests and is thus favorable. 
  

**4. $p$-value**

  * The $p$-values corresponding to each alternative hypothesis are listed in the table below

```{r, echo=FALSE, warning=F, message=F}
set.seed(123)

df = cbind.data.frame(`Alternative Hypothesis` = c('$H_A: \\mu_1 - \\mu_2 \\neq 0$','$H_A: \\mu_1 - \\mu_2 < 0$','$H_A: \\mu_1 - \\mu_2 > 0$'),
                      `$p$-value` = c('$P(|t| \\geq |t_{obs}||H_0)$',
                                             '$P(t \\leq t_{obs}|H_0)$',
                                             '$P(t\\geq t_{obs}|H_0)$'))

kable(df, format = 'html', digits = 2,
      row.names = F, booktabs = T, escape = F)%>%
  kable_classic(full_width = F, html_font = "Cambria")%>%kable_styling(bootstrap_options = 'striped')



```

**5. Decision rule**

  \[\text{Reject} \ \ H_0 \ \ \text{if} \ \ \text{$p$-value} < \alpha \]
  
  

#### The pooled $t$ test

There is one situation where the above test statistic has exactly a $t$-distribution and that occurs when we can assume that both populations have the same standard deviation. In this case, we need only substitute a single sample standard deviation for $\sigma_1$ and $\sigma_2$. We call this the pooled estimate of the standard deviation:

\[ s_{pooled}^2 \frac{(n_1 - 1)s_1^2 + (n_2 -1)s_2^2}{n_1+n_2 - 2}\]

$s_{pooled}^2$ is called the **pooled estimator** of $\sigma_2$ because it combines the information from both samples. 

The test statistic under the assumption of equal variances becomes 

\[t_{obs} = \frac{(\bar{x}_1 - \bar{x}_2) - (\mu_1 - \mu_2)}{s_{pooled}\sqrt{\frac{1}{n_1}+\frac{1}{n_2}}}  \]

which is exactly distributed as a $t$ distribution with $n_1+n_2 - 2$ degrees of freedom. In practice, a significance test for determining equality of variances can be run prior to the $t$-test to determine if the pooled or unpooled test statistic should be used. More information about this test can be found [here]()


**Example: NCAA Womens Basketball** - Sunday April 7th South Carolina and Iowa contended for the NCAA womens basketball championship. Consider the following table which gives the opponent and number of points scored for both teams in their last $15$ games leading up to the championship.

```{r, echo=FALSE, warning=F, message=F}
set.seed(123)

sc.matchup = rev(c('UConn','Tenessee','Georgia','Alabama', 'Kentucky', 'Arkansas', 'Tenessee', 'Texas A&M','Tenessee', 'LSU','Presbyterian', 'UNC', 'Indiana', 'Oregon State','NC State'))
sc = c(78, 70, 79, 88, 91, 79, 74, 79, 76, 98, 103, 72, 70, 66, 83)
spacer = rep('', 15)
io.matchup = rev(c('Penn State','Nebraska','Michigan','Indiana','Illinois','Minnesota', 'Ohio State', 'Penn State', 'Michigan', 'Nebraska', 'Holy Cross', 'West Virginia', 'Colorado', 'LSU', 'UConn'))
io = c(71, 94, 89, 64, 91, 94, 95, 95, 93, 108, 101, 69, 106, 79, 111)

df1 = cbind.data.frame(`South Carolina` = sc,
                       `Matchup` = sc.matchup,
                       `-` = spacer,
                       `Iowa` = io,
                       `Matchup` = io.matchup)

kable(df1, format = 'html', digits = 2,
      row.names = F, booktabs = T, escape = F)%>%
  kable_classic(full_width = F, html_font = "Cambria")%>%kable_styling(bootstrap_options = 'striped')



```


The summary of the last $15$ games is given below:

```{r, echo=FALSE, warning=F, message=F}
set.seed(123)

sc.matchup = rev(c('UConn','Tenessee','Georgia','Alabama', 'Kentucky', 'Arkansas', 'Tenessee', 'Texas A&M','Tenessee', 'LSU','Presbyterian', 'UNC', 'Indiana', 'Oregon State','NC State'))
sc = c(78, 70, 79, 88, 91, 79, 74, 79, 76, 98, 103, 72, 70, 66, 83)
spacer = rep('', 15)
io.matchup = rev(c('Penn State','Nebraska','Michigan','Indiana','Illinois','Minnesota', 'Ohio State', 'Penn State', 'Michigan', 'Nebraska', 'Holy Cross', 'West Virginia', 'Colorado', 'LSU', 'UConn'))
io = c(71, 94, 89, 64, 91, 94, 95, 95, 93, 108, 101, 69, 106, 79, 111)

df = cbind.data.frame(Population = c('Iowa', 'South Carolina'),
                      `Population Mean` = c('$\\mu_1$', '$\\mu_2$'),
                      `Sample Size` = c('$n_1 = 15$', '$n_2 = 15$'),
                      `Sample Mean` = c(round(mean(io), 2), round(mean(sc), 2)),
                      `Sample Stdev` = c(round(sd(io), 2), round(sd(sc), 2)))

kable(df, format = 'html', digits = 2,
      row.names = F, booktabs = T, escape = F)%>%
  kable_classic(full_width = F, html_font = "Cambria")%>%kable_styling(bootstrap_options = 'striped')



```


Conduct a two sample $t$-test at the $\alpha = 0.05$ significance level to determine if there is a significant difference between the championship teams in the mean number of points scored per game.



### A significance test for comparing two means: dependent samples

<br>
<br>
<br>
<br>

### References











































